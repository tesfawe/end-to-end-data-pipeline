{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0ae815-8eba-404b-80cc-a0b44195c532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ebd73e-b1e6-4fe6-9339-8879ce4c2986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789aa8d3-3636-40e4-8d90-bd973cfe3279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS pipeline.bronze_stats (\n",
    "    stage STRING,\n",
    "    file_name STRING,\n",
    "    start_time TIMESTAMP,\n",
    "    read_end_time TIMESTAMP,\n",
    "    end_time TIMESTAMP,\n",
    "    file_size_mb DOUBLE,\n",
    "    num_records_ingested LONG,\n",
    "    read_duration_seconds DOUBLE,\n",
    "    write_duration_seconds DOUBLE,\n",
    "    error_message STRING,\n",
    "    timestamp TIMESTAMP\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a0498e-c6a5-4110-b88e-931c487d647e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(module)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def load_ingestion_config(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the ingestion configuration from a JSON file.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file existence\n",
    "        if not Path(file_path).exists():\n",
    "            error_message = f\"File not found at path: {file_path}\"\n",
    "            logging.error(error_message)\n",
    "            raise FileNotFoundError(error_message)\n",
    "\n",
    "        # Load the JSON file\n",
    "        logging.info(f\"Loading ingestion configuration from file: {file_path}\")\n",
    "        with open(file_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Log successful loading\n",
    "        logging.info(\"Ingestion configuration loaded successfully.\")\n",
    "        return config\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        logging.error(f\"FileNotFoundError occurred: {fnf_error}\")\n",
    "        raise  \n",
    "\n",
    "    except json.JSONDecodeError as json_error:\n",
    "        error_message = f\"Invalid JSON in file: {file_path}. Details: {json_error}\"\n",
    "        logging.error(error_message)\n",
    "        raise ValueError(error_message) from json_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = \"/Workspace/Users/tesfamit03@gmail.com/code/config/files_to_ingest.json\"\n",
    "    try:\n",
    "        files_to_ingest = load_ingestion_config(json_file_path)\n",
    "        logging.info(f\"Ingestion configuration: {files_to_ingest}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load ingestion configuration. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acbf569-4347-45c2-9718-ca3afc30cbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/tesfamit03@gmail.com/code/utils/logging_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d008db3-8f39-41e0-b479-e06c76148b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d0d409-4403-4857-9c5a-e0d4afdb730b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully ingested appearance.csv into bronze.appearance.\nSuccessfully ingested club_games.csv into bronze.club_games.\nSuccessfully ingested clubs.csv into bronze.clubs.\nSuccessfully ingested competitions.csv into bronze.competitions.\nSuccessfully ingested game_events.csv into bronze.game_events.\nSuccessfully ingested game_lineups.csv into bronze.game_lineups.\nSuccessfully ingested games.csv into bronze.games.\nSuccessfully ingested players.csv into bronze.players.\nSuccessfully ingested player_valuations.csv into bronze.player_valuations.\nSuccessfully ingested transfers.csv into bronze.transfers.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(module)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Iterate through each file and ingest it\n",
    "for file_info in files_to_ingest:\n",
    "    file_name = file_info.get(\"file_name\")\n",
    "    file_path = file_info.get(\"file_path\")\n",
    "    table_name = file_info.get(\"table_name\")\n",
    "\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not all([file_name, file_path, table_name]):\n",
    "            error_message = f\"Missing required fields for file: {file_name}. File info: {file_info}\"\n",
    "            logging.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "        logging.info(f\"Starting ingestion process for file: {file_name} into table: {table_name}\")\n",
    "\n",
    "        # Start time\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Read the file\n",
    "        logging.info(f\"Reading file: {file_path}\")\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .load(file_path)\n",
    "\n",
    "        read_end_time = datetime.datetime.now()\n",
    "\n",
    "        # Write to the Bronze Layer\n",
    "        logging.info(f\"Writing file {file_name} to Delta table: {table_name}\")\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        # Calculate file size\n",
    "        file_size_mb = round(sum([file.size for file in dbutils.fs.ls(file_path)]) / (1024 * 1024), 2)\n",
    "\n",
    "        # Log metadata\n",
    "        log_pipeline_stats(\n",
    "            stage=\"ingestion\",\n",
    "            stats={\n",
    "                \"file_name\": file_name,\n",
    "                \"start_time\": start_time,  \n",
    "                \"read_end_time\": read_end_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"file_size_mb\": file_size_mb,\n",
    "                \"num_records_ingested\": df.count(),\n",
    "                \"read_duration_seconds\": (read_end_time - start_time).total_seconds(),\n",
    "                \"write_duration_seconds\": (end_time - read_end_time).total_seconds()\n",
    "            },\n",
    "            table_name=\"pipeline.bronze_stats\"\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Successfully ingested {file_name} into {table_name}.\")\n",
    "        print(f\"Successfully ingested {file_name} into {table_name}.\")\n",
    "    except Exception as e:\n",
    "        # Log errors if ingestion fails\n",
    "        error_message = f\"Failed to ingest file: {file_name}. Error: {str(e)}\"\n",
    "        logging.error(error_message)\n",
    "\n",
    "        log_pipeline_stats(\n",
    "            stage=\"ingestion\",\n",
    "            stats={\n",
    "                \"file_name\": file_name,\n",
    "                \"error_message\": str(e),\n",
    "                \"timestamp\": datetime.datetime.now().isoformat()  # Convert datetime to ISO format\n",
    "            },\n",
    "            table_name=\"pipeline.bronze_stats\"\n",
    "        )\n",
    "\n",
    "        logging.error(f\"Error details: {str(e)}\")\n",
    "        raise RuntimeError(error_message) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b22029-02fe-440e-9cdc-83fed8a9e156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3612831496567460,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion_bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}